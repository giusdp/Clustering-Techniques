\documentclass{llncs}

\usepackage[italian]{babel}
\usepackage[utf8]{inputenc}
% \usepackage{hyperref}
\usepackage{graphicx}

\usepackage{subfig}

\usepackage[T1]{fontenc}

%\usepackage{float}
\usepackage{amssymb}

\author{Giuseppe De Palma}

\title{Progetto Elaborazione Linguaggio Naturale: Tecniche di Clustering}
\institute{Alma Mater Studiorum - Università di Bologna \\
	\email{giuseppe.depalma@studio.unibo.it}\\
	\email{Matricola: 854846}
}

\newcommand{\acapo}{\vspace{0.5\baselineskip}\\}

\begin{document}
    \maketitle
	
	\begin{abstract}
		Ciaone
	\end{abstract}
	   
	\section{Introduzione}
	Il \textit{clustering} (o analisi dei gruppi) è una forma di \textit{machine learning} non supervisionato che permette di raggruppare in \textit{cluster} elementi non annotati
	dati in input. Un cluster è una collezione di oggetti ``simili'' tra loro che sono ``dissimili'' rispetto agli oggetti degli altri cluster. Questo tipo di machine learning è
	ottimo per partizionare un insieme di dati in diverse ``categorie'', quindi poter eseguire diverse analisi ed ottenere nuove informazioni.
	Applicazioni tipiche in cui il clustering viene molto usato è il riconoscimento di email di spam (le email a scopi pubblicitari o di frode), oppure per l'aggregazione di notizie (Google News ne è un esempio).
	\acapo
	Il clustering trova possibili applicazioni anche nel campo dell'elaborazione del linguaggio naturale. Oltre alle nuove possibili analisi
	sui corpora ed al fornire una visualizzazione pittografica delle parole raggrupate, un interessante utilizzo è quello della \textbf{generalizzazione} delle parole.
	\acapo 
	Possiamo considerare i vari cluster delle classi di equivalenza. Per questo motivo, se avessimo un dataset su cui comporre i cluster fatto di frasi e parole, allora si potrebbe assumere che una
	qualche parola che compare in una frase può essere sostituita con un'altra dello stesso cluster lasciando intatta la correttezza della frase. 
	Ad esempio, se avessimo nel nostro dataset ``per Lunedì'', ``per Martedì'', ``per Mercoledì'', ``per Sabato'', ``per Domenica'', senza avere ``per Giovedì'' e ``per Vernedì'', e avessimo un cluster in cui 
	i giorni della settimana sono raggruppati insieme, allora potremmo generalizzare l'utilizzo della preposizione ``per'' con Giovedì e Vernedì.
	\acapo
	Il clustering, quindi, può essere molto utile anche nell'elaborazione del linguaggio naturale. Nel progetto in studio vengono testate le capacità di alcune tecniche di clustering
	da cui si derivano dei risultati per mostrarne le differenze, i pregi e i difetti. I dati utilizzati negli esperimenti, comunque, non sono parti di testo, ma semplici dataset di vettori numerici 2D in modo tale da poter facilmente 
	visualizzare i grafici relativi ai cluster e determinare le caratteristiche di ogni tecnica.
	\subsection{Outline}
	[\textbf{SCRIVERE OUTLINE}]
	\section{Clustering}
	Ci sono numerosi algoritmi per effettuare clustering, ma essi sono classificabili in poche tipologie: il clustering gerarchico e il clustering partizionale.
	Clustering partizionale consiste nell'ottenere dei cluster, di solito in modo iterativo, ma spesso senza determinare una vera relazione tra gli elementi. Si inizia con un insieme di cluster iniziale ed iterativamente
	si riassegnano gli oggetti nei giusti cluster. Il clustering gerarchico, invece, forma un albero (la gerarchia) degli elementi dove un nodo rappresenta un sotto-cluster del nodo padre e le foglie sono i singoli
	oggetti.
	\acapo
	Un'altra importante distinzione tra gli algoritmi di clustering è il \textit{soft clustering} e \textit{hard clustering}. Nel primo caso, ogni oggetto può essere assegnato a più cluster secondo un qualche
	grado di appartenza, mentre nel secondo caso ogni oggetto è assegnato ad un unico cluster. In questo progetto vedremo quattro diversi algoritmi, due della classe di clustering gerarchico, due del clustering partizionale.
	I primi tre eseguono hard clustering mentre l'ultimo soft clustering.
	\acapo
	Di seguito sono elencati i metodi implementati e testati:
	\begin{itemize}
		\item Clustering \textbf{gerarchico}
		\begin{enumerate}
			\item Aggregativo
			\item Divisivo
		\end{enumerate}

		\item Clustering \textbf{partizionale}
		\begin{enumerate}
			\item K-Means
			\item EM (soft clustering)
		\end{enumerate}
	\end{itemize}
	
	\subsection{Gerarchico}
	Andando più in dettaglio sulle diverse tecniche, abbiamo detto che la prima classe di clustering permette di creare degli alberi con i cluster e sotto-cluster. 
	Questo può essere ottenuto con un approcco \textit{bottom-up} che è il clustering \textbf{aggregativo},
	il quale inizia dai singoli oggetti e ne raggruppa i più simili, per poi raggruppare i gruppi più simili e così via, fino ad ottenere un unico gruppo
	che sarà la radice dell'albero. Un altro approccio è quello \textit{top-down}, il clustering \textbf{divisivo}, che in modo inverso dal precedente parte 
	dal gruppo comprendente tutti gli elementi e lo divide in sotto-gruppi in modo da massimizzare la similarità intrinseca dei gruppi, 
	fino ad arrivare ai singoli elementi.
	\acapo
	Un utile grafico che si ottiene da questo tipo di clustering è il cosidetto dendrogramma. 
	[\textbf{METTERE FIGURA DENDROGRAMMA E SPIEGARE}]

	\subsubsection{Aggregativo}
	Questo tipo di clustering è realizzato tramite un algoritmo \textit{greedy} che prende in input un insieme di dati $S$, da cui ogni oggetto è considerato essere un piccolo cluster da un elemento.
	Ad ogni passo l'algoritmo determina i due cluster più simili e li unisce in un nuovo cluster. L'algoritmo termina
	quando il cluster contenente tutti gli elementi di $S$ viene formato, il quale sarà l'unico cluster rimanente.
	I modi in cui si possono determinare la similarità dei cluster sono molteplici. L'algoritmo fa uso di una funzione di similarità per calcolare quanto dei cluster sono simili tra di loro, ce ne sono
	diverse, ad esempio alcune molto utilizzate sono:
	\begin{itemize}
		\item single link: ottiene la similarità di due membri \textbf{più} simili da due cluster diversi, rispettivamente;
		\item complete link: ottiene la similarità dei due membri \textbf{meno} simili da due cluster diversi, rispettivamente;
		\item group-average: ottiene la similarità media tra i membri di due cluster.
	\end{itemize}
	Nonostante le funzioni di similarità possano differire anche ampiamente, una proprietà che tutte devono avere è la monotonia. Per un insieme di dati $S$ e una funzione di similarità $sim$:
	\begin{center}
		$\forall c, c', \hat{c} \subseteq S$. $min(sim(c, c’), sim(c, \hat{c})) \ge sim(c, c’ \cup \hat{c})$
	\end{center}
	Questo perché l'operazione di unione garantisce	di non aumentare la similarità, quindi una funzione che non obbedisce a questa condizione
	rovinerebbe la gerarchia in quanto cluster non simili, piazzati in parti lontane nell'albero, potrebbero
	ritrovarsi ad essere simili in unioni successive e perciò l'essere vicini nell'albero non corrisponderebbe più al concetto di similarità.
	
	[\textbf{CAPIRE QUALE FUNZIONE DI SIMILARITA USO E SCRIVERE}]

	\subsubsection{Divisivo}
	Come per la controparte aggregativa, dietro il clustering divisivo c'è un algoritmo greedy. Invece di iniziare dai singoli elementi, si inizia dal cluster contenente tutti gli oggetti. Ad ogni iterazione
	si determina quale cluster è quello \textbf{meno} coerente e lo si divide in due. Come prima si utilizzano funzioni di similarità poichè due cluster con oggetti simili sono più coerenti
	di cluster con oggetti non simili. Ad esempio, un cluster con molti oggetti identici hanno una coerenza massimale.

	\subsection{Partizionale}
	\subsubsection{K-Means}
	\subsubsection{EM}
    \section{Sessione Sperimentale}
    \section{Conclusioni}
    
\end{document}